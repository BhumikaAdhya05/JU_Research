# ğŸ§  Self-prompting Large Vision Models for Few-Shot Medical Image Segmentation

> **Authors:** Qi Wu, Yuyao Zhang, Marawan Elbatel  
> **Conference:** DART 2023 (LNCS 14293)  
> [ğŸ“œ Official Paper DOI](https://doi.org/10.1007/978-3-031-45857-6_16)  
> [ğŸ’» Code](https://github.com/PeterYYZhang/few-shot-self-prompt-SAM)

---

## ğŸ“Œ Motivation

- Annotating medical images is expensive and time-consuming.
- The goal is to achieve **few-shot medical image segmentation** by leveraging the **promptable nature of the Segment Anything Model (SAM)**.
- The core idea: **self-generate prompts** using a **lightweight logistic regression classifier** over SAMâ€™s image embeddings.

---

## ğŸ§  Key Contributions

- A **training-free (or very light)** self-prompting unit is proposed to generate SAM-compatible prompts using only a few labeled samples.
- Demonstrated **superior performance** over SAM fine-tuning baselines like MedSAM and SAMed under few-shot constraints.
- Can be trained in **< 30 seconds on GPU** or seconds on CPU, making it practical in clinical settings.

---

## ğŸ—ï¸ Architecture Overview

1. Input image is passed through **frozen SAM encoder** â†’ generates 64Ã—64Ã—256 feature embeddings.
2. These embeddings are input to a **logistic regression** classifier (1Ã—1) â†’ outputs **coarse segmentation mask**.
3. Using this mask:
   - A **bounding box** is generated via morphological ops.
   - A **location point** is extracted using **distance transform**.
4. These **prompts (box + point)** are then passed into SAM decoder â†’ **final segmentation** output.

---

## ğŸ§® Training Objective

Each pixel is classified using **logistic regression**.

Let:

- `t_{mn}`: ground truth pixel value at position (m, n)
- `Å·_{mn}`: sigmoid output of the logistic regression at (m, n)

The **pixel-wise cross-entropy loss** is:

$$
L = -\frac{1}{k} \sum_q \sum_{m,n} \left[ t_{mn} \log(\hat{y}_{mn}) + (1 - t_{mn}) \log(1 - \hat{y}_{mn}) \right]
$$

- $t_{mn}$: Ground truth pixel value at position $(m, n)$  
- $\hat{y}_{mn}$: Sigmoid output of the logistic regression at $(m, n)$

Where:
- `k`: number of training images
- `q`: index of image in training set

---

## ğŸ§ª Experiments

### ğŸ—‚ï¸ Datasets

- **Kvasir-SEG**: Polyp segmentation, 1000 images
- **ISIC-2018**: Skin lesion segmentation, 2594 images

### âš™ï¸ Implementation

- **Backbone**: SAM ViT-B (smallest version)
- **Classifier**: Logistic Regression (scikit-learn, default params)
- **Training**: 5-fold cross-validation
- **Post-processing**: 
  - 3Ã— Erosion â†’ 5Ã— Dilation using 5Ã—5 kernel
  - Resize output mask to 256Ã—256
- **Prompt Generation**: Morphology + distance transform

---

## ğŸ“Š Results

### ğŸ¯ Quantitative (20-shot)

| Model               | Kvasir Dice | ISIC Dice |
|--------------------|-------------|------------|
| **Ours (Point+Box)** | **62.78%**  | **66.78%** |
| MedSAM [16]         | 55.01%      | 64.94%     |
| SAMed [29]          | 61.48%      | 63.27%     |
| SAM-B (unprompted)  | 52.66%      | 45.25%     |
| Prompted SAM-B (GT) | 86.86%      | 84.28%     |
| U-Net (full-data)   | 88.10%      | 88.36%     |

---

## ğŸ§ª Ablation Studies

### ğŸ“Œ Number of Shots (ISIC-2018)

| Shots | Linear | Point | Box | Point+Box |
|-------|--------|-------|-----|-----------|
| 10    | 58.81  | 59.49 | 47.13 | 64.22     |
| 20    | 62.69  | 61.23 | 47.99 | 66.78     |
| 40    | 63.81  | 61.52 | 49.81 | 67.88     |
| Full  | 65.62  | 61.76 | 55.03 | 69.51     |

---

## âš–ï¸ Prompt Type Impact

- **Point** â†’ Helps accurately locate center of object, lacks size/shape info.
- **Box** â†’ Conveys rough region and size but can miss fine localization.
- **Combined** â†’ Achieves the best performance by combining both precision and scope.

---

## âš ï¸ Limitations

- âŒ **Multi-instance segmentation**: Struggles to handle multiple objects in an image.
- âŒ **Modality-specific generalization**: SAM decoder underperforms on **ultrasound** due to sensitivity to high-frequency noise.
- âŒ **Decoder bottleneck**: SAM's decoder isn't trained on medical modalities, limiting full potential.

---

## ğŸ’¡ Future Directions

- âœ… Incorporate **modality-specific fine-tuning** (e.g., MedSAM decoder adapters).
- âœ… Use **iterative prompt refinement** for better multi-stage segmentation.
- âœ… Replace logistic regression with **lightweight CNNs or attention-based modules**.
- âœ… Explore **multi-instance prompt generation** techniques (clustering, region growing).

---

## ğŸ“ References

- MedSAM: [arXiv:2304.12306](https://arxiv.org/abs/2304.12306)
- SAMed: [arXiv:2304.13785](https://arxiv.org/abs/2304.13785)
- SAM (Segment Anything): [arXiv:2304.02643](https://arxiv.org/abs/2304.02643)

---

## ğŸ”— Useful Links

- ğŸ“„ [Springer DOI](https://doi.org/10.1007/978-3-031-45857-6_16)
- ğŸ’» [GitHub Repository](https://github.com/PeterYYZhang/few-shot-self-prompt-SAM)

---

## ğŸ§¾ Citation

```bibtex
@inproceedings{wu2023selfprompting,
  title={Self-prompting Large Vision Models for Few-Shot Medical Image Segmentation},
  author={Wu, Qi and Zhang, Yuyao and Elbatel, Marawan},
  booktitle={DART 2023},
  year={2024},
  publisher={Springer}
}
```

# ğŸ§  Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation

> A simplified and detailed explanation of the paper by Qi Wu, Yuyao Zhang, and Marawan Elbatel  
> Presented at DART 2023 ([Paper Link](https://doi.org/10.1007/978-3-031-45857-6_16))

---

## ğŸš¨ The Problem

Medical image segmentation â€” like finding tumors in MRI or polyps in endoscopy â€” needs:
- âœ… Highly accurate models
- âŒ Lots of **manual annotations** by medical experts
- âŒ Expensive labeling effort

But what if we only have **a few labeled images** (few-shot)? How do we still make a model that segments well?

---

## ğŸ§ª The Solution: Self-Prompting SAM

### What's SAM?

- SAM = **Segment Anything Model** from Meta AI
- Itâ€™s trained on huge datasets of **natural images**
- It can segment objects **if you give it prompts**, like:
  - A **point** inside the object
  - A **bounding box** around the object

But hereâ€™s the issue:
> SAM needs **good prompts** to work well. And in medical imaging, itâ€™s hard to provide these manually for each new scan.

### So what do the authors propose?

> âœ¨ Train a tiny model to **generate those prompts** automatically â€” using just a few labeled images â€” and feed them to SAM.

---

## ğŸ› ï¸ Step-by-Step: How It Works

### ğŸ”’ 1. Freeze SAM

- Donâ€™t touch SAMâ€™s internal parameters.
- Use it **as-is**: image encoder + prompt encoder + mask decoder.

---

### ğŸ§  2. Add a Tiny Self-Prompting Module

- Input an image into SAMâ€™s ViT encoder â†’ get **64Ã—64Ã—256** feature embeddings
- Add a **simple logistic regression classifier** (just a linear layer!)
  - It predicts: for each pixel, is it inside the object? (Yes = 1, No = 0)
- This gives a **rough mask** â€” not perfect, but enough to hint where the object is.

---

### ğŸ§¾ 3. Extract Prompts from the Rough Mask

From the coarse binary mask:
- ğŸŸ¡ **Point Prompt**:
  - Use **distance transform** to find the pixel farthest from the boundary (i.e. safely inside the object)
- ğŸ”² **Box Prompt**:
  - Find the smallest box that covers the mask
  - Clean the mask with **morphological operations** (erosion + dilation)

---

### ğŸ¯ 4. Use Prompts with SAM

- Combine the **point** and **box** with the original image
- Feed these into SAM's **mask decoder**
- ğŸ§  SAM uses its pre-trained power to generate a **precise segmentation mask**

---

## ğŸ¤– What Makes This Special?

### âœ… Works with Very Little Data

- Only needs **10â€“20 labeled images** to perform well
- No need to train or fine-tune the big SAM model

### âœ… Ultra-Lightweight

- The classifier is just a **single linear layer**
- Whole training takes **<30 seconds on a GPU**, or a few seconds on CPU!

### âœ… Beats Other Fine-Tuning Methods in Few-Shot Settings

- Performs better than MedSAM and SAMed when both use the same small dataset
- Without touching SAMâ€™s weights at all

---

## ğŸ“Š Performance Summary (20-shot setting)

| Model               | Kvasir Dice | ISIC Dice |
|--------------------|-------------|------------|
| Ours (point + box) | **62.78%**  | **66.78%** |
| MedSAM             | 55.01%      | 64.94%     |
| SAMed              | 61.48%      | 63.27%     |
| SAM (no prompts)   | 52.66%      | 45.25%     |
| Full-data U-Net    | 88.10%      | 88.36%     |

---

## ğŸ”¬ Why This Works (Intuitively)

> Think of the model as two brains working together:

### ğŸ§  Brain 1: "Rough Estimator" (the self-prompt module)

- Learns **basic shape and location** of the object from just a few images
- Not perfect, but cheap and fast

### ğŸ§  Brain 2: "Expert Segmenter" (SAM)

- Given a decent prompt (box + point), it uses its **deep knowledge from millions of natural images** to do the real work

**Result:** High-quality segmentation using very little training data.

---

## âš–ï¸ Limitations

- âŒ Doesnâ€™t handle **multiple objects** in one image well
- âŒ Struggles with **noisy modalities** like ultrasound
- âŒ SAM's decoder isn't trained for medical data â€” can still be a bottleneck

---

## ğŸ’¡ Future Improvements

- Combine with **modality-tuned decoders** for better accuracy
- Replace linear classifier with a **lightweight CNN**
- Improve **multi-instance prompting**
- Explore **iterative refinement**: better prompts â†’ better masks â†’ better prompts...

---

## ğŸ§  Summary: What You Should Remember

- You can use a **frozen giant model** (SAM) and teach a **tiny helper** to give it rough instructions
- This works surprisingly well â€” even in **few-shot settings**
- Self-prompting turns **coarse predictions** into **high-quality segmentations**
- Itâ€™s **simple**, **fast**, and **effective** for real-world medical scenarios

---


