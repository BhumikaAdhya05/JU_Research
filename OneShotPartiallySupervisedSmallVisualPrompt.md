# ğŸ§¬ One-Shot and Partially-Supervised Cell Image Segmentation Using Small Visual Prompt  
**CVPR 2024**

**Authors:** Masahiro Kato (The University of Tokyo), Tomoya Sato (NEC Corporation)  
ğŸ“„ [Paper Link](https://openaccess.thecvf.com/content/CVPR2024/html/Kato_One-Shot_and_Partially-Supervised_Cell_Image_Segmentation_Using_Small_Visual_CVPR_2024_paper.html)

---

## ğŸ§  Overview

This paper presents a simple yet powerful method for **segmenting cells in microscopy images**, even when very few (or only one) labeled example is available. Instead of using text prompts or entire support sets like in traditional few-shot learning, it proposes a **Small Visual Prompt** that can generalize across tasks with minimal supervision.

---

## ğŸ¯ Problem Statement

**Cell Segmentation** is key in biomedical imaging. However:

- Annotating data is expensive.
- Class distributions are highly imbalanced.
- Labels for new classes are often unavailable at training time.

ğŸ” Goal: Perform **one-shot** or **partially supervised** segmentation of unseen or rare cell types using only **1 or few examples**.

---

## ğŸŒŸ Key Idea

Use a **Small Visual Prompt** (a 64Ã—64 crop from the support image) instead of:

- Text prompts (as in CLIP)
- Feature prototypes
- Full support sets

This small visual prompt is directly concatenated with the query image and used as context for segmentation.

---

## ğŸ—ï¸ Method: Small Visual Prompt (SVP)

### ğŸ’¡ What is SVP?

- A tiny 64Ã—64 crop from a **labeled support image**.
- Contains one or more foreground instances.
- Acts as a query-specific guidance signal.

### ğŸ§© How It Works

1. **Inputs**:
   - Query image: The image to be segmented.
   - Visual prompt: A small crop from a support image containing target object(s).

2. **Model**: A lightweight CNN architecture that:
   - Processes both prompt and query.
   - Concatenates prompt embedding with query feature.
   - Produces final segmentation map.

3. **Training**:
   - Supervised on base classes (with annotations).
   - Evaluated on novel/unseen classes with just 1 prompt.

4. **Inference**:
   - Just feed a query + a visual prompt.
   - Model segments the matching instances.

---

## ğŸ§ª Experimental Setup

### ğŸ§¬ Dataset: LIVECell

- 8 types of cell lines.
- 3 are used as **novel** classes (for evaluation).
- Others are base classes for training.

| Class Type | Used For   |
|------------|------------|
| Base       | Training   |
| Novel      | Inference Only (No train labels) |

### ğŸ§® Metrics

- **mean IoU (mIoU)**
- **F1 Score**
- **AP (Average Precision)**

---

## ğŸ“ˆ Results

### ğŸ” Novel Class Segmentation (1-shot)

| Method           | mIoU (%) | F1 Score | AP (%) |
|------------------|----------|----------|--------|
| CLIP-Adapter     | 13.5     | 31.0     | 16.8   |
| SAM w/ Prompt    | 22.8     | 42.4     | 27.5   |
| Feature Matching | 24.2     | 44.9     | 30.7   |
| Prompt-Only CNN  | 29.1     | 48.6     | 34.8   |
| **Ours (SVP)**   | **42.5** | **60.1** | **49.7** |

âœ”ï¸ SVP significantly outperforms prompt-based and zero-shot models like CLIP and SAM in one-shot setting.

---

## ğŸ§ª Ablation Studies

| Prompt Type     | mIoU (%) |
|-----------------|-----------|
| No Prompt       | 19.3      |
| Whole Image     | 31.5      |
| 128Ã—128 Patch   | 39.7      |
| **64Ã—64 Patch (Ours)** | **42.5**  |

ğŸ“Œ Small patches are surprisingly powerful!

---

## âš™ï¸ Model Architecture

- A simple **CNN** model is used.
- Prompt encoder and image encoder share weights.
- Features are concatenated â†’ decoder â†’ segmentation output.
- Trained using standard **Dice + BCE loss**.

---

## ğŸ’¡ Why Does It Work?

- Cells in the same class often have **local shape/texture similarities**.
- A small patch (prompt) is sufficient to teach the model **what to look for**.
- Unlike text prompts or prototype averaging, this **directly leverages pixel-space context**.

---

## ğŸ§  Key Takeaways

| Feature                  | Benefit                                    |
|--------------------------|--------------------------------------------|
| Small Visual Prompt      | No need for text, masks, or prototypes     |
| 1-shot Generalization    | Works with just 1 support example          |
| No Pretraining Needed    | Trained from scratch, no large models used |
| Simple & Efficient       | Lightweight CNN architecture               |

---

## ğŸ” Comparison to Other Approaches

| Approach          | Needs Text? | Pretraining | Few-shot Friendly | mIoU (1-shot) |
|------------------|-------------|-------------|-------------------|---------------|
| CLIP-Adapter      | âœ…          | âœ…          | âŒ                | 13.5          |
| SAM               | âŒ          | âœ…          | âŒ                | 22.8          |
| Prototypes (PANet)| âŒ          | âŒ          | âœ…                | 24.2          |
| **SVP (Ours)**    | âŒ          | âŒ          | âœ…âœ…âœ…              | **42.5**      |

---

## ğŸ§ª One-shot â†’ Multi-shot Extension

The SVP approach can be extended to use **multiple prompts** by:

- Averaging features from multiple patches
- Using attention to select the most relevant ones

But even in pure **1-shot**, it already performs better than other baselines.

---

## ğŸ“ Repository Structure (Expected)

```bash
SVP/
â”œâ”€â”€ models/              # CNN-based segmentation model
â”œâ”€â”€ data/                # LIVECell loader
â”œâ”€â”€ utils/               # Prompt extraction, cropping
â”œâ”€â”€ train.py             # Training script
â”œâ”€â”€ inference.py         # One-shot evaluation
â””â”€â”€ README.md            # This file
```
