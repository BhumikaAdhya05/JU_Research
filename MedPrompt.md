# ğŸ“Œ MedPrompt: Label-Efficient Medical Segmentation Using Text-Only Supervision

**Authors**: Qi Zhang, Yufei Ye, Li Zhang  
**ArXiv ID**: [2410.12562v1](https://arxiv.org/abs/2410.12562)  
**Date**: Oct 18, 2023

---

## ğŸ§  What Is This Paper About?

**MedPrompt** introduces a new approach for **medical image segmentation** that **doesnâ€™t need image annotations at all**.  
Instead, it learns **just from text** â€” using a few class descriptions like:

> â€œLiver is a large organ located in the upper right abdomen.â€  

This paper shows that text-only training (zero-pixel labels!) can teach a model to segment organs and tumors with decent accuracy.

---

## ğŸš¨ Why This Matters

- Labeling medical images is **costly** and **requires domain experts**.
- Prior methods still need **some pixel labels**.
- MedPrompt can:
  - Be trained with **only text supervision**
  - Achieve **promising segmentation performance**
  - Serve as a strong starting point for **label-efficient medical AI**

---

## ğŸ” Key Idea

> Train a segmentation model by **aligning image regions with text prompts** using **CLIP-style contrastive learning**.

### Inputs:
- **Text Prompts**: Class descriptions (e.g., â€œLiver is an organâ€¦â€).
- **Images**: No annotations or segmentation masks.

### Goal:
Learn image-to-text alignment and use it to segment classes described by the text.

---

## ğŸ—ï¸ Architecture Overview

### 1. **Image Encoder**  
- A vision transformer (e.g., ViT) extracts patch-level features from images.

### 2. **Text Encoder**  
- A frozen CLIP text encoder processes textual class descriptions.
- Output is a global feature vector per class.

### 3. **Prompt Alignment**  
- Use **region-text contrastive learning** to match image patches to text prompts.
- Patches similar to the text prompt get higher weights.

### 4. **Prompt-Conditioned Mask Decoder**  
- A decoder takes the weighted features and generates a segmentation mask.
- Learns **class-specific masks** without needing mask labels.

---

## ğŸ› ï¸ Training Strategy

### Input:
- A batch of images (unlabeled)
- Text prompts for known categories

### Optimization:
- Contrastive loss between image patches and class text embeddings
- Mask loss (for any few available samples in semi-supervised case)

### Zero-shot:
- Entire model trained only on text
- Evaluated on segmentation task directly

---

## ğŸ§ª Datasets and Results

### Datasets:
- **Synapse**: Multi-organ CT segmentation
- **BTCV**: 13 abdominal structures
- **MSD (Liver, Colon)**: Tumor segmentation

### Results Summary (Zero-Shot Setting):

| Dataset  | mIoU (Zero-Shot) | mDice (Zero-Shot) |
|----------|------------------|-------------------|
| Synapse  | 35.2%            | 43.5%             |
| BTCV     | 37.0%            | 45.1%             |
| Liver    | 38.7%            | 47.3%             |
| Colon    | 28.4%            | 36.2%             |

> ğŸ“Œ These are **achieved without any segmentation labels** during training!

---

## ğŸ§ª Semi-Supervised Setting (Few Labels)

If a few labeled masks are available (1%â€“10%):
- MedPrompt **finetunes** using its pre-trained weights
- Outperforms standard semi-supervised models

---

## ğŸ’¬ Text Prompt Examples

| Class     | Prompt                                                  |
|-----------|----------------------------------------------------------|
| Liver     | â€œLiver is the largest internal organ in the human body.â€ |
| Kidney    | â€œThe kidney filters blood to produce urine.â€             |
| Colon     | â€œThe colon is part of the large intestine.â€              |

> These simple prompts are enough to guide segmentation.

---

## ğŸ”¬ Loss Functions

### 1. **Contrastive Loss**:
Matches patch embeddings to text embeddings.

### 2. **Segmentation Loss** (optional):
Used only when labeled masks are available.

---

## ğŸ“ Suggested Repo Structure

```bash
MedPrompt/
â”œâ”€â”€ models/               # Encoders and decoder modules
â”œâ”€â”€ prompts/              # Class description text files
â”œâ”€â”€ datasets/             # Synapse, BTCV, MSD loaders
â”œâ”€â”€ train_zero_shot.py    # Zero-shot training script
â”œâ”€â”€ train_finetune.py     # Semi-supervised finetuning
â”œâ”€â”€ inference.py          # Run segmentation on test images
â””â”€â”€ README.md             # You're here!
```

# ğŸ“˜ MedPrompt â€“ A Simple Explanation of the Paper

**Title**: MedPrompt: Label-Efficient Medical Segmentation Using Text-Only Supervision
**Authors**: Qi Zhang, Yufei Ye, Li Zhang
**Published**: arXiv, October 2023 ([arXiv:2410.12562](https://arxiv.org/abs/2410.12562))

---

## ğŸ§  What Is MedPrompt?

MedPrompt is a **new way to do medical image segmentation** without any manual mask labels.

Instead of drawing organ or tumor boundaries, MedPrompt:

* Uses **only short text descriptions** (like â€œThe liver is the largest organ in the abdomenâ€).
* Learns to match these descriptions to image regions.

You can segment medical images by just describing what you want to find. âœ¨

---

## ğŸ’¡ Why Is This Important?

Medical image labeling is expensive:

* Requires doctors or radiologists
* Takes hours per scan

MedPrompt offers:

* **Zero-shot segmentation** (no pixel labels during training)
* Label-efficient learning
* Easier deployment across medical tasks

---

## ğŸ” How It Works (In Simple Terms)

MedPrompt uses **three key components**:

### 1. ğŸ–¼ï¸ Image Encoder

* A vision model (e.g. ViT) extracts features from the image
* It breaks the image into **patches** and generates a vector for each one

### 2. ğŸ“ Text Encoder (from CLIP)

* Text prompts like "The kidney filters blood" are turned into a **text vector**
* This encoder is **frozen** (not trained)

### 3. ğŸ”„ Contrastive Alignment

* The model learns to make **image patches** similar to the correct **text prompt**
* Patches that match the text prompt get **higher scores**

### 4. ğŸ§© Mask Decoder

* Combines patch scores to predict a **segmentation mask**
* Produces the output: where the target organ/tumor is in the image

---

## ğŸ§ª Training Setup

### Input:

* Images with **no annotations**
* A few **text descriptions** for each class

### Loss:

* **Contrastive Loss**: Match image patches to text embeddings
* **Mask Loss**: Only used in semi-supervised mode (if labeled masks available)

MedPrompt is trained **end-to-end** using just these inputs.

---

## âœï¸ Example Text Prompts Used

| Organ/Tumor | Prompt Text                                     |
| ----------- | ----------------------------------------------- |
| Liver       | "Liver is the largest organ in the body."       |
| Kidney      | "Kidney filters blood to produce urine."        |
| Colon Tumor | "Colon tumor is an abnormal mass in the colon." |

You can write your own descriptions too â€” they donâ€™t have to be perfect!

---

## ğŸ“Š Results â€“ Zero-Shot Performance

| Dataset | mIoU (%) | mDice (%) |
| ------- | -------- | --------- |
| Synapse | 35.2     | 43.5      |
| BTCV    | 37.0     | 45.1      |
| Liver   | 38.7     | 47.3      |
| Colon   | 28.4     | 36.2      |

ğŸ“Œ These results are achieved **without any segmentation masks** used in training.

---

## ğŸ“ˆ Semi-Supervised Performance (With 1â€“10% Labels)

When a few labels are added, MedPrompt can be finetuned:

* Improves performance rapidly
* Outperforms existing semi-supervised models

So, MedPrompt is great for both **zero-shot** and **few-shot** scenarios.

---

## ğŸ”§ Suggested Project Folder Structure

```bash
MedPrompt/
â”œâ”€â”€ models/               # Vision encoder, mask decoder
â”œâ”€â”€ prompts/              # Text descriptions of classes
â”œâ”€â”€ data/                 # Loaders for Synapse, BTCV, MSD
â”œâ”€â”€ train_zero_shot.py    # Train using only text
â”œâ”€â”€ train_finetune.py     # Train using some labels
â”œâ”€â”€ inference.py          # Run model on test images
â””â”€â”€ README.md             # This file
```

---

## ğŸ§  Why Does This Work?

* Vision transformers represent image patches like words
* Text prompts define **what to look for**
* By aligning the two, the model can find relevant areas in images

Itâ€™s like teaching the model: â€œHereâ€™s what a liver is. Go find it.â€

---

## ğŸ“š Citation

```bibtex
@article{zhang2023medprompt,
  title={MedPrompt: Label-Efficient Medical Segmentation Using Text-Only Supervision},
  author={Zhang, Qi and Ye, Yufei and Zhang, Li},
  journal={arXiv preprint arXiv:2410.12562},
  year={2023}
}
```

---

## âœ… TL;DR

| ğŸ§  What is it?  | Medical segmentation with just text prompts |
| --------------- | ------------------------------------------- |
| ğŸ¯ Why use it?  | No masks needed, great for low-label setups |
| ğŸ”¬ How it works | Contrast image patches with text embeddings |
| ğŸ§ª Results      | Strong zero-shot + semi-supervised accuracy |

---


